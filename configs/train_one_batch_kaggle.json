{
    "name": "fastspeech_kaggle",
    "n_gpu": 1,
    "arch": {
        "type": "HiFiGANModel",
        "args": {
            "generator_args": {
                "h_u": 512,
                "k_u": [16, 16, 4, 4],
                "k_r": [3, 7, 11],
                "D_r": [
                    [[1, 1], [3, 1], [5, 1]], 
                    [[1, 1], [3, 1], [5, 1]], 
                    [[1, 1], [3, 1], [5, 1]]
                ]
            },
            "mpd_periods": [2, 3, 5, 7, 11],
            "msd_args": {
                "channels_list": [128, 128, 256, 512, 1024, 1024, 1024, 1],
                "kernels": [15, 41, 41, 41, 41, 41, 5, 3],
                "strides": [1, 2, 2, 4, 4, 1, 1, 1],
                "groups_list": [1, 4, 16, 16, 16, 16, 1, 1]
           }
        }
    },
    "data": {
        "train": {
            "batch_size": 2,
            "num_workers": 1,
            "datasets": [
                {
                    "type": "LJSpeechDataset",
                    "args": {
                        "dir": "/kaggle/input/ljspeech/LJSpeech-1.1",
                        "limit": 8
                    }
                }
            ]
        },
        "val": {
            "batch_size": 1,
            "num_workers": 1,
            "datasets": [
                {
                    "type": "LJSpeechDataset",
                    "args": {
                        "dir": "test_model/test_data"
                    }
                }
            ]
        }
    },
    "loss": {
        "type": "HiFiGANLoss",
        "args": {
            "lambda_fm": 2,
            "lambda_mel": 45
        }
    },
    "gen_optimizer": {
        "type": "Adam",
        "args": {
            "lr": 2e-4,
            "betas": [0.8, 0.99]
        }
    },
    "gen_lr_scheduler": {
        "type": "ExponentialLR",
        "args": {
            "gamma": 0.999
        }
    },
    "disc_optimizer": {
        "type": "Adam",
        "args": {
            "lr": 2e-4,
            "betas": [0.8, 0.99]
        }
    },
    "disc_lr_scheduler": {
        "type": "ExponentialLR",
        "args": {
            "gamma": 0.999
        }
    },
    "trainer": {
        "epochs": 20,
        "len_epoch": 50,
        "log_step": 10,
        "save_dir": "saved/",
        "save_period": 1,
        "verbosity": 2,
        "monitor": "min val_loss",
        "early_stop": 100,
        "visualize": "wandb",
        "wandb_project": "dla4_one_batch",
        "grad_norm_clip": 5
    }
}
